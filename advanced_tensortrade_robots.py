#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Ä–æ–±–æ—Ç—ã –Ω–∞ –±–∞–∑–µ TensorTrade —Å –∞–Ω–∞–ª–∏–∑–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π, CNN –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –∫ –æ–±—É—á–µ–Ω–∏—é
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞: https://github.com/WISEPLAT/Hackathon-Finam-NN-Trade-Robot
"""

import os
import sys
import json
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import requests
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# TensorTrade –∏–º–ø–æ—Ä—Ç—ã
try:
    import tensortrade as tt
    from tensortrade.environments import TradingEnvironment
    from tensortrade.data import DataFeed, Stream
    from tensortrade.actions import DiscreteActionStrategy
    from tensortrade.rewards import SimpleProfitStrategy
    from tensortrade.exchanges import Exchange, ExchangeOptions
    from tensortrade.instruments import USD, BTC, ETH
    from tensortrade.wallets import Wallet, Portfolio
    TENSORTRADE_AVAILABLE = True
except ImportError:
    TENSORTRADE_AVAILABLE = False
    print("‚ö†Ô∏è TensorTrade –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tensortrade")

# ML –∏–º–ø–æ—Ä—Ç—ã
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Concatenate
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("‚ö†Ô∏è TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tensorflow")

# NLP –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('advanced_trading_robots.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NewsSentimentAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.sentiment_pipeline = None
        self.financial_pipeline = None
        self.news_cache = {}
        
        if TRANSFORMERS_AVAILABLE:
            try:
                # –û–±—â–∞—è –º–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
                self.sentiment_pipeline = pipeline(
                    "sentiment-analysis",
                    model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                    return_all_scores=True
                )
                
                # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                self.financial_pipeline = pipeline(
                    "text-classification",
                    model="ProsusAI/finbert",
                    return_all_scores=True
                )
                
                logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π: {e}")
    
    def analyze_sentiment(self, text: str, use_financial: bool = True) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not text or not self.sentiment_pipeline:
            return {'positive': 0.5, 'negative': 0.5, 'neutral': 0.0}
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            pipeline_to_use = self.financial_pipeline if use_financial and self.financial_pipeline else self.sentiment_pipeline
            
            results = pipeline_to_use(text[:512])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            sentiment_scores = {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}
            
            for result in results[0]:
                label = result['label'].lower()
                score = result['score']
                
                if 'positive' in label or 'bullish' in label:
                    sentiment_scores['positive'] = score
                elif 'negative' in label or 'bearish' in label:
                    sentiment_scores['negative'] = score
                else:
                    sentiment_scores['neutral'] = score
            
            return sentiment_scores
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è: {e}")
            return {'positive': 0.5, 'negative': 0.5, 'neutral': 0.0}
    
    def get_market_news(self, symbols: List[str], days_back: int = 7) -> Dict[str, List[Dict]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
        # –ù–∞–ø—Ä–∏–º–µ—Ä, NewsAPI, Alpha Vantage News, –∏–ª–∏ Finam API
        
        news_data = {}
        
        for symbol in symbols:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            sample_news = [
                {
                    'title': f'{symbol} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç –Ω–∞ —Ñ–æ–Ω–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤',
                    'content': f'–ê–∫—Ü–∏–∏ {symbol} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–æ—Å—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è —É–ª—É—á—à–µ–Ω–∏—é —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π...',
                    'published_at': datetime.now() - timedelta(hours=np.random.randint(1, 24*days_back)),
                    'source': 'Financial News'
                },
                {
                    'title': f'–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ–≤—ã—à–∞—é—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ {symbol}',
                    'content': f'–í–µ–¥—É—â–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ {symbol} –≤ —Å—Ç–æ—Ä–æ–Ω—É –ø–æ–≤—ã—à–µ–Ω–∏—è...',
                    'published_at': datetime.now() - timedelta(hours=np.random.randint(1, 24*days_back)),
                    'source': 'Market Analysis'
                }
            ]
            
            news_data[symbol] = sample_news
        
        return news_data
    
    def calculate_sentiment_score(self, news_list: List[Dict]) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
        if not news_list:
            return 0.0
        
        total_sentiment = 0.0
        weight_sum = 0.0
        
        for news in news_list:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            title_sentiment = self.analyze_sentiment(news['title'])
            content_sentiment = self.analyze_sentiment(news['content'])
            
            # –í–∑–≤–µ—à–∏–≤–∞–µ–º (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞–∂–Ω–µ–µ)
            title_weight = 0.7
            content_weight = 0.3
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–∏–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç
            sentiment = (title_sentiment['positive'] - title_sentiment['negative']) * title_weight + \
                       (content_sentiment['positive'] - content_sentiment['negative']) * content_weight
            
            # –í–∑–≤–µ—à–∏–≤–∞–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤–∞–∂–Ω–µ–µ)
            hours_ago = (datetime.now() - news['published_at']).total_seconds() / 3600
            time_weight = max(0.1, 1.0 - hours_ago / (24 * 7))  # –ó–∞ –Ω–µ–¥–µ–ª—é –≤–µ—Å –ø–∞–¥–∞–µ—Ç –¥–æ 0.1
            
            total_sentiment += sentiment * time_weight
            weight_sum += time_weight
        
        return total_sentiment / weight_sum if weight_sum > 0 else 0.0

class CNNPatternClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é CNN"""
    
    def __init__(self, input_shape: Tuple[int, int] = (60, 5)):
        self.input_shape = input_shape
        self.model = None
        self.scaler = StandardScaler()
        self.pattern_types = ['bullish_flag', 'bearish_flag', 'head_shoulders', 'double_top', 'double_bottom', 'triangle', 'channel']
        
    def create_cnn_model(self) -> Model:
        """–°–æ–∑–¥–∞–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        input_layer = Input(shape=self.input_shape, name='price_data')
        
        # CNN —Å–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(input_layer)
        conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(conv1)
        pool1 = MaxPooling1D(pool_size=2)(conv1)
        
        conv2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(pool1)
        conv2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(conv2)
        pool2 = MaxPooling1D(pool_size=2)(conv2)
        
        conv3 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(pool2)
        conv3 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(conv3)
        pool3 = MaxPooling1D(pool_size=2)(conv3)
        
        # Flatten –∏ Dense —Å–ª–æ–∏
        flatten = Flatten()(pool3)
        dense1 = Dense(256, activation='relu')(flatten)
        dropout1 = Dropout(0.5)(dense1)
        dense2 = Dense(128, activation='relu')(dropout1)
        dropout2 = Dropout(0.3)(dense2)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        pattern_output = Dense(len(self.pattern_types), activation='softmax', name='pattern_classification')(dropout2)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã
        direction_output = Dense(3, activation='softmax', name='direction_prediction')(dropout2)  # up, down, sideways
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –¥–≤—É–º—è –≤—ã—Ö–æ–¥–∞–º–∏
        model = Model(inputs=input_layer, outputs=[pattern_output, direction_output])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss={
                'pattern_classification': 'categorical_crossentropy',
                'direction_prediction': 'categorical_crossentropy'
            },
            loss_weights={'pattern_classification': 0.7, 'direction_prediction': 0.3},
            metrics=['accuracy']
        )
        
        return model
    
    def prepare_training_data(self, df: pd.DataFrame, window_size: int = 60) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è CNN"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        price_columns = ['open', 'high', 'low', 'close', 'volume']
        price_data = df[price_columns].values
        normalized_data = self.scaler.fit_transform(price_data)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω
        X = []
        pattern_labels = []
        direction_labels = []
        
        for i in range(window_size, len(normalized_data) - 1):
            # –û–∫–Ω–æ —Ü–µ–Ω
            window = normalized_data[i-window_size:i]
            X.append(window)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            pattern_label = self._generate_pattern_label(df.iloc[i-window_size:i])
            pattern_labels.append(pattern_label)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            direction_label = self._generate_direction_label(df.iloc[i], df.iloc[i+1])
            direction_labels.append(direction_label)
        
        return np.array(X), np.array(pattern_labels), np.array(direction_labels)
    
    def _generate_pattern_label(self, window_df: pd.DataFrame) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        
        label = np.zeros(len(self.pattern_types))
        pattern_idx = np.random.randint(0, len(self.pattern_types))
        label[pattern_idx] = 1.0
        
        return label
    
    def _generate_direction_label(self, current_row: pd.Series, next_row: pd.Series) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–æ–∫ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        price_change = (next_row['close'] - current_row['close']) / current_row['close']
        
        label = np.zeros(3)  # [up, down, sideways]
        
        if price_change > 0.01:  # –†–æ—Å—Ç > 1%
            label[0] = 1.0  # up
        elif price_change < -0.01:  # –ü–∞–¥–µ–Ω–∏–µ > 1%
            label[1] = 1.0  # down
        else:
            label[2] = 1.0  # sideways
        
        return label
    
    def train_model(self, X: np.ndarray, pattern_labels: np.ndarray, direction_labels: np.ndarray, 
                   epochs: int = 100, validation_split: float = 0.2) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏"""
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        self.model = self.create_cnn_model()
        
        # Callbacks
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ModelCheckpoint('best_cnn_model.h5', save_best_only=True)
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = self.model.fit(
            X,
            {'pattern_classification': pattern_labels, 'direction_prediction': direction_labels},
            epochs=epochs,
            validation_split=validation_split,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        return history.history
    
    def predict_patterns(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        predictions = self.model.predict(X)
        pattern_predictions = predictions[0]
        direction_predictions = predictions[1]
        
        return pattern_predictions, direction_predictions

class UnsupervisedLearningAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
    
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.clusterer = KMeans(n_clusters=5, random_state=42)
        self.pca = PCA(n_components=10)
        self.scaler = StandardScaler()
        self.is_trained = False
    
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df = df.copy()
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        df['sma_5'] = df['close'].rolling(5).mean()
        df['sma_20'] = df['close'].rolling(20).mean()
        df['sma_50'] = df['close'].rolling(50).mean()
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        
        # Volume indicators
        df['volume_sma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        # Price changes
        df['price_change_1'] = df['close'].pct_change(1)
        df['price_change_5'] = df['close'].pct_change(5)
        df['price_change_20'] = df['close'].pct_change(20)
        
        # Volatility
        df['volatility'] = df['close'].rolling(20).std()
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_columns = [
            'sma_5', 'sma_20', 'sma_50', 'rsi', 'bb_width',
            'volume_ratio', 'price_change_1', 'price_change_5', 
            'price_change_20', 'volatility'
        ]
        
        features = df[feature_columns].dropna()
        return features.values
    
    def train_unsupervised_models(self, df: pd.DataFrame):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        logger.info("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = self.prepare_features(df)
        
        if len(features) < 100:
            logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è")
            return
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        features_scaled = self.scaler.fit_transform(features)
        
        # PCA –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        features_pca = self.pca.fit_transform(features_scaled)
        
        # –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π
        self.anomaly_detector.fit(features_pca)
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        self.clusterer.fit(features_pca)
        
        self.is_trained = True
        logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –æ–±—É—á–µ–Ω—ã")
    
    def detect_anomalies(self, df: pd.DataFrame) -> np.ndarray:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã")
        
        features = self.prepare_features(df)
        features_scaled = self.scaler.transform(features)
        features_pca = self.pca.transform(features_scaled)
        
        anomalies = self.anomaly_detector.predict(features_pca)
        return anomalies
    
    def get_market_regimes(self, df: pd.DataFrame) -> np.ndarray:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã")
        
        features = self.prepare_features(df)
        features_scaled = self.scaler.transform(features)
        features_pca = self.pca.transform(features_scaled)
        
        clusters = self.clusterer.predict(features_pca)
        return clusters

class AdvancedTradingRobot:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π —Ä–æ–±–æ—Ç —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, symbols: List[str], initial_capital: float = 100000):
        self.symbols = symbols
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.positions = {}
        self.trades = []
        self.equity_history = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.sentiment_analyzer = NewsSentimentAnalyzer()
        self.pattern_classifier = CNNPatternClassifier()
        self.unsupervised_agent = UnsupervisedLearningAgent()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        self.sentiment_weight = 0.3
        self.pattern_weight = 0.4
        self.technical_weight = 0.3
        
        logger.info("ü§ñ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π —Ä–æ–±–æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def load_historical_data(self, symbol: str, days: int = 365) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MOEX API
        base_url = "https://iss.moex.com/iss/"
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        all_data = []
        current_date = start_date
        batch_size = 90
        
        while current_date < end_date:
            batch_end = min(current_date + timedelta(days=batch_size), end_date)
            
            try:
                url = f"{base_url}engines/stock/markets/shares/boards/TQBR/securities/{symbol}/candles.json"
                params = {
                    'from': current_date.strftime('%Y-%m-%d'),
                    'till': batch_end.strftime('%Y-%m-%d'),
                    'interval': 1
                }
                
                response = requests.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    candles = data['candles']['data']
                    
                    if candles:
                        df_batch = pd.DataFrame(candles, columns=[
                            'open', 'close', 'high', 'low', 'value', 'volume', 'begin', 'end'
                        ])
                        df_batch['begin'] = pd.to_datetime(df_batch['begin'])
                        df_batch['end'] = pd.to_datetime(df_batch['end'])
                        all_data.append(df_batch)
                
                current_date = batch_end
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö {symbol}: {e}")
                current_date = batch_end
        
        if all_data:
            df = pd.concat(all_data, ignore_index=True)
            df = df.sort_values('begin').reset_index(drop=True)
            logger.info(f"‚úÖ {symbol}: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return df
        else:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}")
            return pd.DataFrame()
    
    def train_all_models(self, training_data: Dict[str, pd.DataFrame]):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è
        all_data = []
        for symbol, df in training_data.items():
            all_data.append(df)
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è
            self.unsupervised_agent.train_unsupervised_models(combined_df)
        
        # –û–±—É—á–µ–Ω–∏–µ CNN –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        for symbol, df in training_data.items():
            if len(df) > 200:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö
                logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ CNN –¥–ª—è {symbol}...")
                
                try:
                    X, pattern_labels, direction_labels = self.pattern_classifier.prepare_training_data(df)
                    
                    if len(X) > 50:
                        history = self.pattern_classifier.train_model(X, pattern_labels, direction_labels, epochs=50)
                        logger.info(f"‚úÖ CNN –¥–ª—è {symbol} –æ–±—É—á–µ–Ω–∞")
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CNN {symbol}")
                        
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è CNN –¥–ª—è {symbol}: {e}")
    
    def generate_trading_signal(self, symbol: str, df: pd.DataFrame, 
                              sentiment_score: float) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
        try:
            # 1. –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ CNN
            pattern_signal = 0.0
            if self.pattern_classifier.model is not None:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
                recent_data = df.tail(60)
                if len(recent_data) >= 60:
                    X = self.pattern_classifier.prepare_training_data(recent_data)[0]
                    if len(X) > 0:
                        pattern_pred, direction_pred = self.pattern_classifier.predict_patterns(X[-1:])
                        
                        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                        direction_probs = direction_pred[0]
                        pattern_signal = direction_probs[0] - direction_probs[1]  # up - down
            
            # 2. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –∏ —Ä–µ–∂–∏–º–æ–≤
            anomaly_signal = 0.0
            regime_signal = 0.0
            
            if self.unsupervised_agent.is_trained:
                anomalies = self.unsupervised_agent.detect_anomalies(df.tail(100))
                regimes = self.unsupervised_agent.get_market_regimes(df.tail(100))
                
                # –ê–Ω–æ–º–∞–ª–∏–∏ –º–æ–≥—É—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ä–∞–∑–≤–æ—Ä–æ—Ç
                if len(anomalies) > 0 and anomalies[-1] == -1:
                    anomaly_signal = 0.5  # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–∞–∑–≤–æ—Ä–æ—Ç
                
                # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∂–∏–º–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
                if len(regimes) > 0:
                    current_regime = regimes[-1]
                    # –†–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
                    regime_signal = (current_regime - 2) * 0.2  # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –≤–æ–∫—Ä—É–≥ 0
            
            # 3. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            technical_signal = self._calculate_technical_signals(df)
            
            # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–∏–≥–Ω–∞–ª—ã
            final_signal = (
                sentiment_score * self.sentiment_weight +
                pattern_signal * self.pattern_weight +
                technical_signal * self.technical_weight +
                anomaly_signal * 0.1 +
                regime_signal * 0.1
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            if final_signal > 0.3:
                action = 'buy'
                confidence = min(final_signal, 1.0)
            elif final_signal < -0.3:
                action = 'sell'
                confidence = min(abs(final_signal), 1.0)
            else:
                action = 'hold'
                confidence = 0.0
            
            return {
                'action': action,
                'confidence': confidence,
                'final_signal': final_signal,
                'components': {
                    'sentiment': sentiment_score,
                    'pattern': pattern_signal,
                    'technical': technical_signal,
                    'anomaly': anomaly_signal,
                    'regime': regime_signal
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è {symbol}: {e}")
            return {'action': 'hold', 'confidence': 0.0, 'final_signal': 0.0}
    
    def _calculate_technical_signals(self, df: pd.DataFrame) -> float:
        """–†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
        if len(df) < 20:
            return 0.0
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        current_rsi = rsi.iloc[-1]
        
        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        macd = ema_12 - ema_26
        signal_line = macd.ewm(span=9).mean()
        
        # Bollinger Bands
        sma_20 = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        bb_upper = sma_20 + (bb_std * 2)
        bb_lower = sma_20 - (bb_std * 2)
        
        current_price = df['close'].iloc[-1]
        current_macd = macd.iloc[-1]
        current_signal = signal_line.iloc[-1]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã
        signal = 0.0
        
        # RSI —Å–∏–≥–Ω–∞–ª—ã
        if current_rsi < 30:
            signal += 0.3  # –ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å
        elif current_rsi > 70:
            signal -= 0.3  # –ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å
        
        # MACD —Å–∏–≥–Ω–∞–ª—ã
        if current_macd > current_signal and macd.iloc[-2] <= signal_line.iloc[-2]:
            signal += 0.2  # –ó–æ–ª–æ—Ç–æ–π –∫—Ä–µ—Å—Ç
        elif current_macd < current_signal and macd.iloc[-2] >= signal_line.iloc[-2]:
            signal -= 0.2  # –ú–µ—Ä—Ç–≤—ã–π –∫—Ä–µ—Å—Ç
        
        # Bollinger Bands —Å–∏–≥–Ω–∞–ª—ã
        if current_price < bb_lower.iloc[-1]:
            signal += 0.2  # –û—Ç—Å–∫–æ–∫ –æ—Ç –Ω–∏–∂–Ω–µ–π –ø–æ–ª–æ—Å—ã
        elif current_price > bb_upper.iloc[-1]:
            signal -= 0.2  # –û—Ç—Å–∫–æ–∫ –æ—Ç –≤–µ—Ä—Ö–Ω–µ–π –ø–æ–ª–æ—Å—ã
        
        return signal
    
    def backtest_strategy(self, test_data: Dict[str, pd.DataFrame], 
                         news_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """–ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        logger.info("üìä –ù–∞—á–∏–Ω–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥...")
        
        results = {}
        
        for symbol in self.symbols:
            if symbol not in test_data:
                continue
            
            df = test_data[symbol]
            news = news_data.get(symbol, [])
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç
            sentiment_score = self.sentiment_analyzer.calculate_sentiment_score(news)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            capital = self.initial_capital
            position = 0
            trades = []
            equity_history = []
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –¥–∞–Ω–Ω—ã–º
            for i in range(60, len(df)):  # –ù–∞—á–∏–Ω–∞–µ–º —Å 60-–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è CNN
                current_data = df.iloc[:i+1]
                current_price = df['close'].iloc[i]
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª
                signal = self.generate_trading_signal(symbol, current_data, sentiment_score)
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ—Ä–≥–æ–≤–ª—é
                if signal['action'] == 'buy' and position == 0 and signal['confidence'] > 0.5:
                    position = capital / current_price
                    capital = 0
                    trades.append({
                        'type': 'buy',
                        'price': current_price,
                        'time': df['begin'].iloc[i],
                        'confidence': signal['confidence']
                    })
                elif signal['action'] == 'sell' and position > 0 and signal['confidence'] > 0.5:
                    capital = position * current_price
                    position = 0
                    trades.append({
                        'type': 'sell',
                        'price': current_price,
                        'time': df['begin'].iloc[i],
                        'confidence': signal['confidence']
                    })
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è
                current_equity = capital + (position * current_price if position > 0 else 0)
                equity_history.append({
                    'time': df['begin'].iloc[i],
                    'equity': current_equity,
                    'price': current_price
                })
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏—é
            if position > 0:
                final_price = df['close'].iloc[-1]
                capital = position * final_price
                trades.append({
                    'type': 'sell',
                    'price': final_price,
                    'time': df['begin'].iloc[-1],
                    'confidence': 0.0
                })
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            final_equity = capital
            total_return = (final_equity - self.initial_capital) / self.initial_capital * 100
            
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
            equity_series = pd.Series([h['equity'] for h in equity_history])
            rolling_max = equity_series.expanding().max()
            drawdown = (equity_series - rolling_max) / rolling_max * 100
            max_drawdown = drawdown.min()
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            returns = equity_series.pct_change().dropna()
            volatility = returns.std() * np.sqrt(252 * 24 * 60) * 100
            
            # Sharpe ratio
            risk_free_rate = 0.05
            excess_returns = returns - risk_free_rate / (252 * 24 * 60)
            sharpe_ratio = excess_returns.mean() / returns.std() * np.sqrt(252 * 24 * 60) if returns.std() > 0 else 0
            
            results[symbol] = {
                'total_return': total_return,
                'max_drawdown': max_drawdown,
                'volatility': volatility,
                'sharpe_ratio': sharpe_ratio,
                'total_trades': len(trades),
                'trades': trades,
                'equity_history': equity_history,
                'sentiment_score': sentiment_score
            }
            
            logger.info(f"‚úÖ {symbol}: –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å={total_return:.2f}%, –ü—Ä–æ—Å–∞–¥–∫–∞={max_drawdown:.2f}%, Sharpe={sharpe_ratio:.2f}")
        
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ü–†–û–î–í–ò–ù–£–¢–´–• –¢–û–†–ì–û–í–´–• –†–û–ë–û–¢–û–í")
    logger.info("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫
    if not TENSORTRADE_AVAILABLE:
        logger.warning("‚ö†Ô∏è TensorTrade –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é")
    
    if not TENSORFLOW_AVAILABLE:
        logger.warning("‚ö†Ô∏è TensorFlow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, CNN —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥—É—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã")
    
    if not TRANSFORMERS_AVAILABLE:
        logger.warning("‚ö†Ô∏è Transformers –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –±—É–¥–µ—Ç —É–ø—Ä–æ—â–µ–Ω")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    symbols = ['GAZP', 'SBER', 'PIKK', 'IRAO', 'SGZH']
    training_days = 365
    test_days = 90
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–æ–±–æ—Ç–∞
    robot = AdvancedTradingRobot(symbols)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ...")
    all_data = {}
    
    for symbol in symbols:
        df = robot.load_historical_data(symbol, training_days + test_days)
        if not df.empty:
            all_data[symbol] = df
    
    if not all_data:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    training_data = {}
    test_data = {}
    
    for symbol, df in all_data.items():
        split_point = int(len(df) * (training_days / (training_days + test_days)))
        training_data[symbol] = df.iloc[:split_point]
        test_data[symbol] = df.iloc[split_point:]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    robot.train_all_models(training_data)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    logger.info("üì∞ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏...")
    news_data = robot.sentiment_analyzer.get_market_news(symbols)
    
    # –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥
    results = robot.backtest_strategy(test_data, news_data)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    with open('advanced_trading_results.json', 'w', encoding='utf-8') as f:
        # –û—á–∏—â–∞–µ–º –æ—Ç —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
        def clean_for_json(obj):
            if isinstance(obj, dict):
                return {k: clean_for_json(v) for k, v in obj.items() 
                       if k not in ['model', 'scaler', 'anomaly_detector', 'clusterer']}
            elif isinstance(obj, list):
                return [clean_for_json(item) for item in obj]
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif hasattr(obj, 'isoformat'):
                return obj.isoformat()
            else:
                return obj
        
        clean_results = clean_for_json(results)
        json.dump(clean_results, f, ensure_ascii=False, indent=2)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–î–í–ò–ù–£–¢–´–• –†–û–ë–û–¢–û–í:")
    logger.info("=" * 60)
    
    for symbol, result in results.items():
        logger.info(f"{symbol}:")
        logger.info(f"  üìà –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {result['total_return']:.2f}%")
        logger.info(f"  üìâ –ú–∞–∫—Å. –ø—Ä–æ—Å–∞–¥–∫–∞: {result['max_drawdown']:.2f}%")
        logger.info(f"  üìä Sharpe ratio: {result['sharpe_ratio']:.2f}")
        logger.info(f"  üîÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {result['total_trades']}")
        logger.info(f"  üòä –°–µ–Ω—Ç–∏–º–µ–Ω—Ç: {result['sentiment_score']:.3f}")
        logger.info("")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    returns = [r['total_return'] for r in results.values()]
    drawdowns = [r['max_drawdown'] for r in results.values()]
    sharpe_ratios = [r['sharpe_ratio'] for r in results.values()]
    
    logger.info("üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    logger.info(f"  –°—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {np.mean(returns):.2f}%")
    logger.info(f"  –°—Ä–µ–¥–Ω—è—è –ø—Ä–æ—Å–∞–¥–∫–∞: {np.mean(drawdowns):.2f}%")
    logger.info(f"  –°—Ä–µ–¥–Ω–∏–π Sharpe: {np.mean(sharpe_ratios):.2f}")
    
    logger.info(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ advanced_trading_results.json")
    logger.info("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")

if __name__ == "__main__":
    main()
