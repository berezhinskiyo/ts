#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ TensorTrade
"""

import os
import sys
import json
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import requests
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ML –∏–º–ø–æ—Ä—Ç—ã
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, Input
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("‚ö†Ô∏è TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Ç–∏–ø–æ–≤
    class Model:
        pass
    class Sequential:
        pass

# NLP –∏–º–ø–æ—Ä—Ç—ã
try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleSentimentAnalyzer:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    
    def __init__(self):
        self.sentiment_pipeline = None
        if TRANSFORMERS_AVAILABLE:
            try:
                self.sentiment_pipeline = pipeline("sentiment-analysis")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def analyze_sentiment(self, text: str) -> float:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not text or not self.sentiment_pipeline:
            return 0.0
        
        try:
            result = self.sentiment_pipeline(text[:512])
            label = result[0]['label']
            score = result[0]['score']
            
            if 'POSITIVE' in label:
                return score
            elif 'NEGATIVE' in label:
                return -score
            else:
                return 0.0
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è: {e}")
            return 0.0
    
    def get_sample_news(self, symbol: str) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        return [
            {
                'title': f'{symbol} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç',
                'content': f'–ê–∫—Ü–∏–∏ {symbol} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É',
                'sentiment': 0.7
            },
            {
                'title': f'–ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –æ–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω—ã –ø–æ {symbol}',
                'content': f'–≠–∫—Å–ø–µ—Ä—Ç—ã –¥–∞—é—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –ø–æ {symbol}',
                'sentiment': 0.8
            }
        ]

class SimpleCNNClassifier:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π CNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
    
    def create_model(self, input_shape: Tuple[int, int] = (60, 5)) -> Model:
        """–°–æ–∑–¥–∞–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏"""
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        model = Sequential([
            Conv1D(32, 3, activation='relu', input_shape=input_shape),
            MaxPooling1D(2),
            Conv1D(64, 3, activation='relu'),
            MaxPooling1D(2),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(3, activation='softmax')  # up, down, sideways
        ])
        
        model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])
        return model
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        price_data = df[['open', 'high', 'low', 'close', 'volume']].values
        normalized_data = self.scaler.fit_transform(price_data)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω
        X, y = [], []
        window_size = 60
        
        for i in range(window_size, len(normalized_data) - 1):
            window = normalized_data[i-window_size:i]
            X.append(window)
            
            # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            current_price = normalized_data[i, 3]  # close
            next_price = normalized_data[i+1, 3]
            
            if next_price > current_price * 1.01:
                y.append([1, 0, 0])  # up
            elif next_price < current_price * 0.99:
                y.append([0, 1, 0])  # down
            else:
                y.append([0, 0, 1])  # sideways
        
        return np.array(X), np.array(y)
    
    def train(self, df: pd.DataFrame, epochs: int = 50):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if not TENSORFLOW_AVAILABLE:
            return
        
        X, y = self.prepare_data(df)
        if len(X) < 100:
            logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return
        
        self.model = self.create_model()
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        split = int(len(X) * 0.8)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=32,
            verbose=0
        )
        
        logger.info(f"‚úÖ CNN –æ–±—É—á–µ–Ω–∞. –¢–æ—á–Ω–æ—Å—Ç—å: {history.history['val_accuracy'][-1]:.3f}")
    
    def predict(self, df: pd.DataFrame) -> float:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        if self.model is None:
            return 0.0
        
        try:
            X, _ = self.prepare_data(df)
            if len(X) == 0:
                return 0.0
            
            prediction = self.model.predict(X[-1:], verbose=0)
            direction_probs = prediction[0]
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∏–≥–Ω–∞–ª: up - down
            return direction_probs[0] - direction_probs[1]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return 0.0

class SimpleUnsupervisedAgent:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
    
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.1)
        self.clusterer = KMeans(n_clusters=3)
        self.scaler = StandardScaler()
        self.is_trained = False
    
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = []
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df = df.copy()
        df['sma_20'] = df['close'].rolling(20).mean()
        df['rsi'] = self._calculate_rsi(df['close'])
        df['volatility'] = df['close'].rolling(20).std()
        df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_cols = ['sma_20', 'rsi', 'volatility', 'volume_ratio']
        for col in feature_cols:
            if col in df.columns:
                features.append(df[col].fillna(0).values)
        
        if features:
            return np.column_stack(features)
        return np.array([])
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def train(self, df: pd.DataFrame):
        """–û–±—É—á–µ–Ω–∏–µ"""
        features = self.prepare_features(df)
        if len(features) < 100:
            logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è")
            return
        
        features_scaled = self.scaler.fit_transform(features)
        
        self.anomaly_detector.fit(features_scaled)
        self.clusterer.fit(features_scaled)
        
        self.is_trained = True
        logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –æ–±—É—á–µ–Ω—ã")
    
    def detect_anomalies(self, df: pd.DataFrame) -> bool:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π"""
        if not self.is_trained:
            return False
        
        features = self.prepare_features(df)
        if len(features) == 0:
            return False
        
        features_scaled = self.scaler.transform(features)
        anomalies = self.anomaly_detector.predict(features_scaled)
        
        return len(anomalies) > 0 and anomalies[-1] == -1

class AdvancedTradingRobot:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π —Ä–æ–±–æ—Ç"""
    
    def __init__(self, symbols: List[str], initial_capital: float = 100000):
        self.symbols = symbols
        self.initial_capital = initial_capital
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.sentiment_analyzer = SimpleSentimentAnalyzer()
        self.cnn_classifier = SimpleCNNClassifier()
        self.unsupervised_agent = SimpleUnsupervisedAgent()
        
        logger.info("ü§ñ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π —Ä–æ–±–æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def load_data(self, symbol: str, days: int = 365) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}...")
        
        base_url = "https://iss.moex.com/iss/"
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        all_data = []
        current_date = start_date
        batch_size = 90
        
        while current_date < end_date:
            batch_end = min(current_date + timedelta(days=batch_size), end_date)
            
            try:
                url = f"{base_url}engines/stock/markets/shares/boards/TQBR/securities/{symbol}/candles.json"
                params = {
                    'from': current_date.strftime('%Y-%m-%d'),
                    'till': batch_end.strftime('%Y-%m-%d'),
                    'interval': 1
                }
                
                response = requests.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    candles = data['candles']['data']
                    
                    if candles:
                        df_batch = pd.DataFrame(candles, columns=[
                            'open', 'close', 'high', 'low', 'value', 'volume', 'begin', 'end'
                        ])
                        df_batch['begin'] = pd.to_datetime(df_batch['begin'])
                        all_data.append(df_batch)
                
                current_date = batch_end
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {symbol}: {e}")
                current_date = batch_end
        
        if all_data:
            df = pd.concat(all_data, ignore_index=True)
            df = df.sort_values('begin').reset_index(drop=True)
            logger.info(f"‚úÖ {symbol}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return df
        else:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {symbol}")
            return pd.DataFrame()
    
    def train_models(self, training_data: Dict[str, pd.DataFrame]):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üéì –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è
        all_data = []
        for df in training_data.values():
            all_data.append(df)
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            self.unsupervised_agent.train(combined_df)
        
        # –û–±—É—á–µ–Ω–∏–µ CNN –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        for symbol, df in training_data.items():
            if len(df) > 200:
                logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ CNN –¥–ª—è {symbol}...")
                self.cnn_classifier.train(df)
    
    def generate_signal(self, symbol: str, df: pd.DataFrame) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
        try:
            # 1. –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
            news = self.sentiment_analyzer.get_sample_news(symbol)
            sentiment_score = np.mean([n['sentiment'] for n in news])
            
            # 2. CNN –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            cnn_signal = self.cnn_classifier.predict(df)
            
            # 3. –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            anomaly_detected = self.unsupervised_agent.detect_anomalies(df)
            anomaly_signal = 0.3 if anomaly_detected else 0.0
            
            # 4. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            technical_signal = self._calculate_technical_signals(df)
            
            # 5. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–∏–≥–Ω–∞–ª—ã
            final_signal = (
                sentiment_score * 0.3 +
                cnn_signal * 0.4 +
                technical_signal * 0.2 +
                anomaly_signal * 0.1
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            if final_signal > 0.3:
                action = 'buy'
                confidence = min(final_signal, 1.0)
            elif final_signal < -0.3:
                action = 'sell'
                confidence = min(abs(final_signal), 1.0)
            else:
                action = 'hold'
                confidence = 0.0
            
            return {
                'action': action,
                'confidence': confidence,
                'final_signal': final_signal,
                'components': {
                    'sentiment': sentiment_score,
                    'cnn': cnn_signal,
                    'technical': technical_signal,
                    'anomaly': anomaly_signal
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è {symbol}: {e}")
            return {'action': 'hold', 'confidence': 0.0, 'final_signal': 0.0}
    
    def _calculate_technical_signals(self, df: pd.DataFrame) -> float:
        """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã"""
        if len(df) < 20:
            return 0.0
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        current_rsi = rsi.iloc[-1]
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        signal = 0.0
        
        if current_rsi < 30:
            signal += 0.3  # –ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å
        elif current_rsi > 70:
            signal -= 0.3  # –ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å
        
        # –¢—Ä–µ–Ω–¥
        sma_20 = df['close'].rolling(20).mean()
        if df['close'].iloc[-1] > sma_20.iloc[-1]:
            signal += 0.2
        else:
            signal -= 0.2
        
        return signal
    
    def backtest(self, test_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """–ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥"""
        logger.info("üìä –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥...")
        
        results = {}
        
        for symbol, df in test_data.items():
            capital = self.initial_capital
            position = 0
            trades = []
            equity_history = []
            
            for i in range(60, len(df)):
                current_data = df.iloc[:i+1]
                current_price = df['close'].iloc[i]
                
                signal = self.generate_signal(symbol, current_data)
                
                # –¢–æ—Ä–≥–æ–≤–ª—è
                if signal['action'] == 'buy' and position == 0 and signal['confidence'] > 0.5:
                    position = capital / current_price
                    capital = 0
                    trades.append({'type': 'buy', 'price': current_price, 'time': df['begin'].iloc[i]})
                elif signal['action'] == 'sell' and position > 0 and signal['confidence'] > 0.5:
                    capital = position * current_price
                    position = 0
                    trades.append({'type': 'sell', 'price': current_price, 'time': df['begin'].iloc[i]})
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º equity
                current_equity = capital + (position * current_price if position > 0 else 0)
                equity_history.append({'time': df['begin'].iloc[i], 'equity': current_equity})
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏—é
            if position > 0:
                final_price = df['close'].iloc[-1]
                capital = position * final_price
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            final_equity = capital
            total_return = (final_equity - self.initial_capital) / self.initial_capital * 100
            
            # –ü—Ä–æ—Å–∞–¥–∫–∞
            equity_series = pd.Series([h['equity'] for h in equity_history])
            rolling_max = equity_series.expanding().max()
            drawdown = (equity_series - rolling_max) / rolling_max * 100
            max_drawdown = drawdown.min()
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            returns = equity_series.pct_change().dropna()
            volatility = returns.std() * np.sqrt(252 * 24 * 60) * 100
            
            # Sharpe
            risk_free_rate = 0.05
            excess_returns = returns - risk_free_rate / (252 * 24 * 60)
            sharpe_ratio = excess_returns.mean() / returns.std() * np.sqrt(252 * 24 * 60) if returns.std() > 0 else 0
            
            results[symbol] = {
                'total_return': total_return,
                'max_drawdown': max_drawdown,
                'volatility': volatility,
                'sharpe_ratio': sharpe_ratio,
                'total_trades': len(trades)
            }
            
            logger.info(f"‚úÖ {symbol}: –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å={total_return:.2f}%, –ü—Ä–æ—Å–∞–¥–∫–∞={max_drawdown:.2f}%, Sharpe={sharpe_ratio:.2f}")
        
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–ê–ü–£–°–ö –£–ü–†–û–©–ï–ù–ù–´–• –ü–†–û–î–í–ò–ù–£–¢–´–• –†–û–ë–û–¢–û–í")
    logger.info("=" * 60)
    
    symbols = ['GAZP', 'SBER', 'PIKK', 'IRAO', 'SGZH']
    training_days = 365
    test_days = 90
    
    robot = AdvancedTradingRobot(symbols)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    all_data = {}
    
    for symbol in symbols:
        df = robot.load_data(symbol, training_days + test_days)
        if not df.empty:
            all_data[symbol] = df
    
    if not all_data:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    training_data = {}
    test_data = {}
    
    for symbol, df in all_data.items():
        split_point = int(len(df) * (training_days / (training_days + test_days)))
        training_data[symbol] = df.iloc[:split_point]
        test_data[symbol] = df.iloc[split_point:]
    
    # –û–±—É—á–µ–Ω–∏–µ
    robot.train_models(training_data)
    
    # –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥
    results = robot.backtest(test_data)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    with open('simplified_advanced_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    logger.info("=" * 60)
    
    for symbol, result in results.items():
        logger.info(f"{symbol}:")
        logger.info(f"  üìà –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {result['total_return']:.2f}%")
        logger.info(f"  üìâ –ü—Ä–æ—Å–∞–¥–∫–∞: {result['max_drawdown']:.2f}%")
        logger.info(f"  üìä Sharpe: {result['sharpe_ratio']:.2f}")
        logger.info(f"  üîÑ –°–¥–µ–ª–∫–∏: {result['total_trades']}")
        logger.info("")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    returns = [r['total_return'] for r in results.values()]
    logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {np.mean(returns):.2f}%")
    
    logger.info("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")

if __name__ == "__main__":
    main()
