#!/usr/bin/env python3
"""
–û—Ç–¥–µ–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import os
import sys
import json
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
from tqdm import tqdm
warnings.filterwarnings('ignore')

# ML –∏–º–ø–æ—Ä—Ç—ã
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.svm import SVR
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import joblib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TechnicalIndicators:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
    
    @staticmethod
    def sma(data: pd.Series, window: int) -> pd.Series:
        return data.rolling(window=window).mean()
    
    @staticmethod
    def ema(data: pd.Series, window: int) -> pd.Series:
        return data.ewm(span=window).mean()
    
    @staticmethod
    def rsi(data: pd.Series, window: int = 14) -> pd.Series:
        delta = data.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    @staticmethod
    def macd(data: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
        ema_fast = TechnicalIndicators.ema(data, fast)
        ema_slow = TechnicalIndicators.ema(data, slow)
        macd_line = ema_fast - ema_slow
        signal_line = TechnicalIndicators.ema(macd_line, signal)
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    @staticmethod
    def bollinger_bands(data: pd.Series, window: int = 20, std_dev: float = 2):
        sma = TechnicalIndicators.sma(data, window)
        std = data.rolling(window=window).std()
        upper = sma + (std * std_dev)
        lower = sma - (std * std_dev)
        return upper, sma, lower
    
    @staticmethod
    def atr(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> pd.Series:
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        return tr.rolling(window=window).mean()
    
    @staticmethod
    def create_features(df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        df = df.copy()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–ø–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        total_steps = 8  # –û—Å–Ω–æ–≤–Ω—ã–µ —ç—Ç–∞–ø—ã —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        
        with tqdm(total=total_steps, desc="–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
                 leave=False) as pbar:
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            pbar.set_description("–°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ")
            for window in [5, 10, 20, 50]:
                df[f'sma_{window}'] = TechnicalIndicators.sma(df['close'], window)
                df[f'ema_{window}'] = TechnicalIndicators.ema(df['close'], window)
            pbar.update(1)
            
            # RSI
            pbar.set_description("RSI –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
            df['rsi_14'] = TechnicalIndicators.rsi(df['close'], 14)
            df['rsi_21'] = TechnicalIndicators.rsi(df['close'], 21)
            pbar.update(1)
            
            # MACD
            pbar.set_description("MACD –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä")
            macd, signal, histogram = TechnicalIndicators.macd(df['close'])
            df['macd'] = macd
            df['macd_signal'] = signal
            df['macd_histogram'] = histogram
            pbar.update(1)
            
            # –ü–æ–ª–æ—Å—ã –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞
            pbar.set_description("–ü–æ–ª–æ—Å—ã –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞")
            bb_upper, bb_middle, bb_lower = TechnicalIndicators.bollinger_bands(df['close'])
            df['bb_upper'] = bb_upper
            df['bb_middle'] = bb_middle
            df['bb_lower'] = bb_lower
            df['bb_width'] = (bb_upper - bb_lower) / bb_middle
            df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
            pbar.update(1)
            
            # ATR
            pbar.set_description("ATR –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä")
            df['atr'] = TechnicalIndicators.atr(df['high'], df['low'], df['close'])
            pbar.update(1)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            pbar.set_description("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
            df['price_change'] = df['close'].pct_change()
            df['volume_change'] = df['volume'].pct_change()
            df['high_low_ratio'] = df['high'] / df['low']
            df['close_open_ratio'] = df['close'] / df['open']
            df['body_size'] = abs(df['close'] - df['open']) / df['open']
            pbar.update(1)
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            pbar.set_description("–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å")
            df['volatility_5'] = df['close'].rolling(5).std()
            df['volatility_20'] = df['close'].rolling(20).std()
            pbar.update(1)
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –æ–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            pbar.set_description("–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
            df['hour'] = df['begin'].dt.hour
            df['day_of_week'] = df['begin'].dt.dayofweek
            df['month'] = df['begin'].dt.month
            
            # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df['volume_sma'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma']
            pbar.update(1)
        
        return df

class FastMLTrainer:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.model_scores = {}
        self.models_dir = "trained_models"
        os.makedirs(self.models_dir, exist_ok=True)
    
    def prepare_data(self, df: pd.DataFrame, target_column: str = 'future_return', 
                    lookback: int = 30, forecast_horizon: int = 5) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        df['future_price'] = df['close'].shift(-forecast_horizon)
        df['future_return'] = (df['future_price'] / df['close'] - 1) * 100
        
        # –£–±–∏—Ä–∞–µ–º NaN
        df = df.dropna()
        
        # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_columns = [col for col in numeric_columns if col not in ['future_price', 'future_return', 'value']]
        
        # –°–æ–∑–¥–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ (—É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        X = []
        y = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        data_range = range(lookback, len(df) - forecast_horizon)
        total_windows = len(df) - lookback - forecast_horizon
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω –¥–∞–Ω–Ω—ã—Ö
        with tqdm(total=total_windows, desc="–°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω –¥–∞–Ω–Ω—ã—Ö", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
                 leave=False) as window_pbar:
            
            for i in data_range:
                window_data = df[feature_columns].iloc[i-lookback:i].values
                target_value = df[target_column].iloc[i]
                
                if not np.isnan(target_value):
                    X.append(window_data.flatten())
                    y.append(target_value)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if i % 1000 == 0:
                    window_pbar.update(1000)
                    window_pbar.set_postfix({
                        '–û–∫–æ–Ω': len(X),
                        '–ü—Ä–∏–∑–Ω–∞–∫–æ–≤': len(feature_columns)
                    })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∏—Ç–µ—Ä–∞—Ü–∏–∏
            remaining = total_windows % 1000
            if remaining > 0:
                window_pbar.update(remaining)
        
        return np.array(X), np.array(y), feature_columns
    
    def train_models_fast(self, X: np.ndarray, y: np.ndarray, symbol: str, feature_columns: List[str]):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        logger.info(f"[ML] –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol}...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        logger.info(f"  [DATA] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=False
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        logger.info(f"  [NORM] –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.info(f"  [FEAT] –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {symbol}...")
        feature_selector = SelectKBest(score_func=f_regression, k=min(50, X_train_scaled.shape[1]))  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 30 –¥–æ 50
        X_train_selected = feature_selector.fit_transform(X_train_scaled, y_train)
        X_test_selected = feature_selector.transform(X_test_scaled)
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        models_config = {
            'random_forest': RandomForestRegressor(
                n_estimators=100,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                max_depth=15,      # –£–≤–µ–ª–∏—á–µ–Ω–æ
                min_samples_split=3,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–π –≥–∏–±–∫–æ—Å—Ç–∏
                min_samples_leaf=1,   # –î–æ–±–∞–≤–ª–µ–Ω–æ
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=100,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                learning_rate=0.05,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                max_depth=8,         # –£–≤–µ–ª–∏—á–µ–Ω–æ
                min_samples_split=3, # –î–æ–±–∞–≤–ª–µ–Ω–æ
                random_state=42
            ),
            'ridge': Ridge(alpha=0.1),  # –£–º–µ–Ω—å—à–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            'linear_regression': LinearRegression()
        }
        
        results = {}
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        model_names = list(models_config.keys())
        with tqdm(total=len(model_names), desc=f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π {symbol}", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
            
            for name, model in models_config.items():
                try:
                    pbar.set_description(f"–û–±—É—á–µ–Ω–∏–µ {name} –¥–ª—è {symbol}")
                    start_time = time.time()
                    
                    # –û–±—É—á–µ–Ω–∏–µ
                    if name in ['ridge', 'linear_regression']:
                        model.fit(X_train_selected, y_train)
                        y_pred = model.predict(X_test_selected)
                    else:
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                    
                    training_time = time.time() - start_time
                    
                    # –û—Ü–µ–Ω–∫–∞
                    mse = mean_squared_error(y_test, y_pred)
                    r2 = r2_score(y_test, y_pred)
                    mae = mean_absolute_error(y_test, y_pred)
                    
                    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (–±–æ–ª—å—à–µ —Ñ–æ–ª–¥–æ–≤)
                    if name in ['ridge', 'linear_regression']:
                        cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5, scoring='r2')  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 5
                    else:
                        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 5
                    
                    results[name] = {
                        'model': model,
                        'mse': mse,
                        'r2': r2,
                        'mae': mae,
                        'cv_mean': cv_scores.mean(),
                        'cv_std': cv_scores.std(),
                        'training_time': training_time,
                        'predictions': y_pred,
                        'actual': y_test
                    }
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                    pbar.set_postfix({
                        'R2': f"{r2:.4f}",
                        'MAE': f"{mae:.4f}",
                        '–í—Ä–µ–º—è': f"{training_time:.1f}—Å"
                    })
                    
                    logger.info(f"    [OK] {name}: R2={r2:.4f}, MAE={mae:.4f}, CV={cv_scores.mean():.4f}, –í—Ä–µ–º—è={training_time:.2f}—Å")
                    
                except Exception as e:
                    logger.error(f"    [ERROR] –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {name}: {e}")
                    pbar.set_postfix({'–û—à–∏–±–∫–∞': str(e)[:20]})
                
                pbar.update(1)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"  [SAVE] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è {symbol}...")
        self.models[symbol] = results
        self.scalers[symbol] = scaler
        self.feature_selectors[symbol] = feature_selector
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫
        self.save_models(symbol, results, scaler, feature_selector)
        
        return results
    
    def save_models(self, symbol: str, models: Dict, scaler: StandardScaler, feature_selector: SelectKBest):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        symbol_dir = os.path.join(self.models_dir, symbol)
        os.makedirs(symbol_dir, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        for model_name, model_data in models.items():
            model_path = os.path.join(symbol_dir, f"{model_name}.joblib")
            joblib.dump(model_data['model'], model_path)
            logger.info(f"  [SAVE] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å {model_name} –¥–ª—è {symbol}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –∏ feature_selector
        scaler_path = os.path.join(symbol_dir, "scaler.joblib")
        selector_path = os.path.join(symbol_dir, "feature_selector.joblib")
        
        joblib.dump(scaler, scaler_path)
        joblib.dump(feature_selector, selector_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {name: {k: v for k, v in data.items() if k != 'model'} 
                  for name, data in models.items()}
        
        metrics_path = os.path.join(symbol_dir, "metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2, default=str)
    
    def load_models(self, symbol: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        symbol_dir = os.path.join(self.models_dir, symbol)
        
        if not os.path.exists(symbol_dir):
            logger.warning(f"–ú–æ–¥–µ–ª–∏ –¥–ª—è {symbol} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return {}
        
        models = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        for model_name in ['random_forest', 'gradient_boosting', 'ridge', 'linear_regression']:
            model_path = os.path.join(symbol_dir, f"{model_name}.joblib")
            if os.path.exists(model_path):
                models[model_name] = joblib.load(model_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler –∏ feature_selector
        scaler_path = os.path.join(symbol_dir, "scaler.joblib")
        selector_path = os.path.join(symbol_dir, "feature_selector.joblib")
        
        if os.path.exists(scaler_path):
            self.scalers[symbol] = joblib.load(scaler_path)
        if os.path.exists(selector_path):
            self.feature_selectors[symbol] = joblib.load(selector_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = os.path.join(symbol_dir, "metrics.json")
        if os.path.exists(metrics_path):
            with open(metrics_path, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
                logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {symbol}: {list(metrics.keys())}")
        
        return models

class UnsupervisedTrainer:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
    
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.clusterer = KMeans(n_clusters=5, random_state=42)
        self.pca = PCA(n_components=5)  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.scaler = StandardScaler()
        self.is_trained = False
        self.models_dir = "unsupervised_models"
        os.makedirs(self.models_dir, exist_ok=True)
    
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        feature_columns = [
            'sma_20', 'ema_20', 'rsi_14', 'macd', 'bb_width', 'atr',
            'volatility_20', 'volume_ratio', 'price_change', 'high_low_ratio'
        ]
        
        available_columns = [col for col in feature_columns if col in df.columns]
        
        if not available_columns:
            return np.array([])
        
        features = df[available_columns].fillna(0).values
        return features
    
    def train_unsupervised_models(self, df: pd.DataFrame):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        logger.info("[UNSUPERVISED] –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è
        with tqdm(total=5, desc="–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
            
            pbar.set_description("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            features = self.prepare_features(df)
            if len(features) < 100:
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è")
                return
            pbar.update(1)
            
            pbar.set_description("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
            features_scaled = self.scaler.fit_transform(features)
            pbar.update(1)
            
            pbar.set_description("PCA —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏")
            features_pca = self.pca.fit_transform(features_scaled)
            pbar.update(1)
            
            pbar.set_description("–û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π")
            self.anomaly_detector.fit(features_pca)
            pbar.update(1)
            
            pbar.set_description("–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
            self.clusterer.fit(features_pca)
            pbar.update(1)
        
        self.is_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        logger.info("[SAVE] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è...")
        self.save_models()
        
        logger.info("[OK] –ú–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        joblib.dump(self.anomaly_detector, os.path.join(self.models_dir, "anomaly_detector.joblib"))
        joblib.dump(self.clusterer, os.path.join(self.models_dir, "clusterer.joblib"))
        joblib.dump(self.pca, os.path.join(self.models_dir, "pca.joblib"))
        joblib.dump(self.scaler, os.path.join(self.models_dir, "scaler.joblib"))
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        try:
            self.anomaly_detector = joblib.load(os.path.join(self.models_dir, "anomaly_detector.joblib"))
            self.clusterer = joblib.load(os.path.join(self.models_dir, "clusterer.joblib"))
            self.pca = joblib.load(os.path.join(self.models_dir, "pca.joblib"))
            self.scaler = joblib.load(os.path.join(self.models_dir, "scaler.joblib"))
            self.is_trained = True
            logger.info("[OK] –ú–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except Exception as e:
            logger.warning(f"[WARNING] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è: {e}")

def load_data_from_files(symbols: List[str], data_dir: str = "data/3year_minute_data") -> Dict[str, pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤"""
    logger.info("[LOAD] –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤...")
    
    all_data = {}
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    with tqdm(total=len(symbols), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", 
             bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
        
        for symbol in symbols:
            pbar.set_description(f"–ó–∞–≥—Ä—É–∑–∫–∞ {symbol}")
            file_path = os.path.join(data_dir, f"{symbol}_3year_minute.csv")
            
            if os.path.exists(file_path):
                try:
                    df = pd.read_csv(file_path)
                    df['begin'] = pd.to_datetime(df['begin'])
                    df['end'] = pd.to_datetime(df['end'])
                    
                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    df = df.drop_duplicates(subset=['begin']).reset_index(drop=True)
                    
                    all_data[symbol] = df
                    pbar.set_postfix({
                        '–ó–∞–ø–∏—Å–µ–π': len(df),
                        '–°—Ç–∞—Ç—É—Å': 'OK'
                    })
                    logger.info(f"[OK] {symbol}: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
                    
                except Exception as e:
                    pbar.set_postfix({'–°—Ç–∞—Ç—É—Å': 'ERROR'})
                    logger.error(f"[ERROR] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {symbol}: {e}")
            else:
                pbar.set_postfix({'–°—Ç–∞—Ç—É—Å': 'WARNING'})
                logger.warning(f"[WARNING] –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            pbar.update(1)
    
    return all_data

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    logger.info("[START] –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø ML –ú–û–î–ï–õ–ï–ô")
    logger.info("=" * 50)
    
    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    symbols = ['GAZP', 'SBER', 'PIKK', 'IRAO', 'SGZH']
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    all_data = load_data_from_files(symbols)
    
    if not all_data:
        logger.error("[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    trainer = FastMLTrainer()
    unsupervised_trainer = UnsupervisedTrainer()
    indicators = TechnicalIndicators()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    trained_models = {}
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
    symbols_list = list(all_data.keys())
    with tqdm(total=len(symbols_list), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤", 
             bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as main_pbar:
        
        for symbol, df in all_data.items():
            main_pbar.set_description(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {symbol}")
            logger.info(f"[PROCESS] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {symbol}...")
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            logger.info(f"  [INDICATORS] –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è {symbol}...")
            df_with_features = indicators.create_features(df)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            logger.info(f"  [PREPARE] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è {symbol}...")
            X, y, feature_columns = trainer.prepare_data(df_with_features)
            
            if len(X) > 100:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
                models = trainer.train_models_fast(X, y, symbol, feature_columns)
                trained_models[symbol] = {
                    'models': models,
                    'data': df_with_features
                }
                main_pbar.set_postfix({
                    '–ú–æ–¥–µ–ª–∏': len(models),
                    '–î–∞–Ω–Ω—ã—Ö': f"{len(X)}",
                    '–°—Ç–∞—Ç—É—Å': 'OK'
                })
            else:
                logger.warning(f"[WARNING] {symbol}: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(X)} –∑–∞–ø–∏—Å–µ–π)")
                main_pbar.set_postfix({
                    '–î–∞–Ω–Ω—ã—Ö': f"{len(X)}",
                    '–°—Ç–∞—Ç—É—Å': 'WARNING'
                })
            
            main_pbar.update(1)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è
    if trained_models:
        logger.info("[UNSUPERVISED] –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è...")
        all_training_data = []
        for model_data in trained_models.values():
            all_training_data.append(model_data['data'])
        
        combined_df = pd.concat(all_training_data, ignore_index=True)
        unsupervised_trainer.train_unsupervised_models(combined_df)
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    summary_data = []
    
    for symbol, model_data in trained_models.items():
        models = model_data['models']
        
        for model_name, model_info in models.items():
            summary_data.append({
                'Symbol': symbol,
                'Model': model_name,
                'R2': f"{model_info['r2']:.4f}",
                'MAE': f"{model_info['mae']:.4f}",
                'CV_Mean': f"{model_info['cv_mean']:.4f}",
                'CV_Std': f"{model_info['cv_std']:.4f}",
                'Training_Time': f"{model_info['training_time']:.2f}s"
            })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É
    if summary_data:
        results_df = pd.DataFrame(summary_data)
        results_df.to_csv('model_training_results.csv', index=False)
        
        logger.info("\n[SUMMARY] –°–í–û–î–ö–ê –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô:")
        logger.info("=" * 50)
        print(results_df.to_string(index=False))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        r2_scores = [float(r['R2']) for r in summary_data]
        mae_scores = [float(r['MAE']) for r in summary_data]
        
        logger.info(f"\n[STATS] –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"  –°—Ä–µ–¥–Ω–∏–π R2: {np.mean(r2_scores):.4f}")
        logger.info(f"  –°—Ä–µ–¥–Ω–∏–π MAE: {np.mean(mae_scores):.4f}")
        logger.info(f"  –õ—É—á—à–∏–π R2: {np.max(r2_scores):.4f}")
        logger.info(f"  –•—É–¥—à–∏–π R2: {np.min(r2_scores):.4f}")
        
        # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        logger.info(f"\n[BEST] –õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–û –°–ò–ú–í–û–õ–ê–ú:")
        for symbol in symbols:
            symbol_results = [r for r in summary_data if r['Symbol'] == symbol]
            if symbol_results:
                best_model = max(symbol_results, key=lambda x: float(x['R2']))
                logger.info(f"  {symbol}: {best_model['Model']} (R2={best_model['R2']})")
    
    logger.info(f"\n[SAVE] –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:")
    logger.info(f"  [DIR] trained_models/ - –æ–±—É—á–µ–Ω–Ω—ã–µ ML –º–æ–¥–µ–ª–∏")
    logger.info(f"  [DIR] unsupervised_models/ - –º–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è")
    logger.info(f"  [FILE] model_training_results.csv - —Å–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    logger.info(f"  [FILE] model_training.log - –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
    
    logger.info("\n[COMPLETE] –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main()
