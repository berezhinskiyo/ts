#!/usr/bin/env python3
"""
–û—Ç–¥–µ–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import os
import sys
import json
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# ML –∏–º–ø–æ—Ä—Ç—ã
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.svm import SVR
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import joblib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TechnicalIndicators:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
    
    @staticmethod
    def sma(data: pd.Series, window: int) -> pd.Series:
        return data.rolling(window=window).mean()
    
    @staticmethod
    def ema(data: pd.Series, window: int) -> pd.Series:
        return data.ewm(span=window).mean()
    
    @staticmethod
    def rsi(data: pd.Series, window: int = 14) -> pd.Series:
        delta = data.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    @staticmethod
    def macd(data: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
        ema_fast = TechnicalIndicators.ema(data, fast)
        ema_slow = TechnicalIndicators.ema(data, slow)
        macd_line = ema_fast - ema_slow
        signal_line = TechnicalIndicators.ema(macd_line, signal)
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    @staticmethod
    def bollinger_bands(data: pd.Series, window: int = 20, std_dev: float = 2):
        sma = TechnicalIndicators.sma(data, window)
        std = data.rolling(window=window).std()
        upper = sma + (std * std_dev)
        lower = sma - (std * std_dev)
        return upper, sma, lower
    
    @staticmethod
    def atr(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> pd.Series:
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        return tr.rolling(window=window).mean()
    
    @staticmethod
    def create_features(df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        df = df.copy()
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for window in [5, 10, 20, 50]:
            df[f'sma_{window}'] = TechnicalIndicators.sma(df['close'], window)
            df[f'ema_{window}'] = TechnicalIndicators.ema(df['close'], window)
        
        # RSI
        df['rsi_14'] = TechnicalIndicators.rsi(df['close'], 14)
        df['rsi_21'] = TechnicalIndicators.rsi(df['close'], 21)
        
        # MACD
        macd, signal, histogram = TechnicalIndicators.macd(df['close'])
        df['macd'] = macd
        df['macd_signal'] = signal
        df['macd_histogram'] = histogram
        
        # –ü–æ–ª–æ—Å—ã –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞
        bb_upper, bb_middle, bb_lower = TechnicalIndicators.bollinger_bands(df['close'])
        df['bb_upper'] = bb_upper
        df['bb_middle'] = bb_middle
        df['bb_lower'] = bb_lower
        df['bb_width'] = (bb_upper - bb_lower) / bb_middle
        df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
        
        # ATR
        df['atr'] = TechnicalIndicators.atr(df['high'], df['low'], df['close'])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['price_change'] = df['close'].pct_change()
        df['volume_change'] = df['volume'].pct_change()
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_open_ratio'] = df['close'] / df['open']
        df['body_size'] = abs(df['close'] - df['open']) / df['open']
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df['volatility_5'] = df['close'].rolling(5).std()
        df['volatility_20'] = df['close'].rolling(20).std()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['hour'] = df['begin'].dt.hour
        df['day_of_week'] = df['begin'].dt.dayofweek
        df['month'] = df['begin'].dt.month
        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df['volume_sma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        return df

class FastMLTrainer:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.model_scores = {}
        self.models_dir = "trained_models"
        os.makedirs(self.models_dir, exist_ok=True)
    
    def prepare_data(self, df: pd.DataFrame, target_column: str = 'future_return', 
                    lookback: int = 30, forecast_horizon: int = 5) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        df['future_price'] = df['close'].shift(-forecast_horizon)
        df['future_return'] = (df['future_price'] / df['close'] - 1) * 100
        
        # –£–±–∏—Ä–∞–µ–º NaN
        df = df.dropna()
        
        # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        feature_columns = [col for col in numeric_columns if col not in ['future_price', 'future_return', 'value']]
        
        # –°–æ–∑–¥–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ (—É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        X = []
        y = []
        
        for i in range(lookback, len(df) - forecast_horizon):
            window_data = df[feature_columns].iloc[i-lookback:i].values
            target_value = df[target_column].iloc[i]
            
            if not np.isnan(target_value):
                X.append(window_data.flatten())
                y.append(target_value)
        
        return np.array(X), np.array(y), feature_columns
    
    def train_models_fast(self, X: np.ndarray, y: np.ndarray, symbol: str, feature_columns: List[str]):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        logger.info(f"ü§ñ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {symbol}...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=False
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –ë—ã—Å—Ç—Ä—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_selector = SelectKBest(score_func=f_regression, k=min(30, X_train_scaled.shape[1]))
        X_train_selected = feature_selector.fit_transform(X_train_scaled, y_train)
        X_test_selected = feature_selector.transform(X_test_scaled)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        models_config = {
            'random_forest': RandomForestRegressor(
                n_estimators=50,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                max_depth=10,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=50,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ),
            'ridge': Ridge(alpha=1.0),
            'linear_regression': LinearRegression()
        }
        
        results = {}
        
        for name, model in models_config.items():
            try:
                logger.info(f"  üîÑ –û–±—É—á–∞–µ–º {name}...")
                start_time = time.time()
                
                # –û–±—É—á–µ–Ω–∏–µ
                if name in ['ridge', 'linear_regression']:
                    model.fit(X_train_selected, y_train)
                    y_pred = model.predict(X_test_selected)
                else:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                
                training_time = time.time() - start_time
                
                # –û—Ü–µ–Ω–∫–∞
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                
                # –ë—ã—Å—Ç—Ä–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (–º–µ–Ω—å—à–µ —Ñ–æ–ª–¥–æ–≤)
                if name in ['ridge', 'linear_regression']:
                    cv_scores = cross_val_score(model, X_train_selected, y_train, cv=3, scoring='r2')
                else:
                    cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='r2')
                
                results[name] = {
                    'model': model,
                    'mse': mse,
                    'r2': r2,
                    'mae': mae,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'training_time': training_time,
                    'predictions': y_pred,
                    'actual': y_test
                }
                
                logger.info(f"    ‚úÖ {name}: R¬≤={r2:.4f}, MAE={mae:.4f}, CV={cv_scores.mean():.4f}, –í—Ä–µ–º—è={training_time:.2f}—Å")
                
            except Exception as e:
                logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {name}: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.models[symbol] = results
        self.scalers[symbol] = scaler
        self.feature_selectors[symbol] = feature_selector
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫
        self.save_models(symbol, results, scaler, feature_selector)
        
        return results
    
    def save_models(self, symbol: str, models: Dict, scaler: StandardScaler, feature_selector: SelectKBest):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        symbol_dir = os.path.join(self.models_dir, symbol)
        os.makedirs(symbol_dir, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        for model_name, model_data in models.items():
            model_path = os.path.join(symbol_dir, f"{model_name}.joblib")
            joblib.dump(model_data['model'], model_path)
            logger.info(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å {model_name} –¥–ª—è {symbol}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –∏ feature_selector
        scaler_path = os.path.join(symbol_dir, "scaler.joblib")
        selector_path = os.path.join(symbol_dir, "feature_selector.joblib")
        
        joblib.dump(scaler, scaler_path)
        joblib.dump(feature_selector, selector_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {name: {k: v for k, v in data.items() if k != 'model'} 
                  for name, data in models.items()}
        
        metrics_path = os.path.join(symbol_dir, "metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2, default=str)
    
    def load_models(self, symbol: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        symbol_dir = os.path.join(self.models_dir, symbol)
        
        if not os.path.exists(symbol_dir):
            logger.warning(f"–ú–æ–¥–µ–ª–∏ –¥–ª—è {symbol} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return {}
        
        models = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
        for model_name in ['random_forest', 'gradient_boosting', 'ridge', 'linear_regression']:
            model_path = os.path.join(symbol_dir, f"{model_name}.joblib")
            if os.path.exists(model_path):
                models[model_name] = joblib.load(model_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler –∏ feature_selector
        scaler_path = os.path.join(symbol_dir, "scaler.joblib")
        selector_path = os.path.join(symbol_dir, "feature_selector.joblib")
        
        if os.path.exists(scaler_path):
            self.scalers[symbol] = joblib.load(scaler_path)
        if os.path.exists(selector_path):
            self.feature_selectors[symbol] = joblib.load(selector_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = os.path.join(symbol_dir, "metrics.json")
        if os.path.exists(metrics_path):
            with open(metrics_path, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
                logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {symbol}: {list(metrics.keys())}")
        
        return models

class UnsupervisedTrainer:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
    
    def __init__(self):
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.clusterer = KMeans(n_clusters=5, random_state=42)
        self.pca = PCA(n_components=5)  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.scaler = StandardScaler()
        self.is_trained = False
        self.models_dir = "unsupervised_models"
        os.makedirs(self.models_dir, exist_ok=True)
    
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        feature_columns = [
            'sma_20', 'ema_20', 'rsi_14', 'macd', 'bb_width', 'atr',
            'volatility_20', 'volume_ratio', 'price_change', 'high_low_ratio'
        ]
        
        available_columns = [col for col in feature_columns if col in df.columns]
        
        if not available_columns:
            return np.array([])
        
        features = df[available_columns].fillna(0).values
        return features
    
    def train_unsupervised_models(self, df: pd.DataFrame):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        logger.info("ü§ñ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è...")
        
        features = self.prepare_features(df)
        if len(features) < 100:
            logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è")
            return
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        features_scaled = self.scaler.fit_transform(features)
        
        # PCA –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        features_pca = self.pca.fit_transform(features_scaled)
        
        # –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π
        self.anomaly_detector.fit(features_pca)
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        self.clusterer.fit(features_pca)
        
        self.is_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self.save_models()
        
        logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        joblib.dump(self.anomaly_detector, os.path.join(self.models_dir, "anomaly_detector.joblib"))
        joblib.dump(self.clusterer, os.path.join(self.models_dir, "clusterer.joblib"))
        joblib.dump(self.pca, os.path.join(self.models_dir, "pca.joblib"))
        joblib.dump(self.scaler, os.path.join(self.models_dir, "scaler.joblib"))
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è"""
        try:
            self.anomaly_detector = joblib.load(os.path.join(self.models_dir, "anomaly_detector.joblib"))
            self.clusterer = joblib.load(os.path.join(self.models_dir, "clusterer.joblib"))
            self.pca = joblib.load(os.path.join(self.models_dir, "pca.joblib"))
            self.scaler = joblib.load(os.path.join(self.models_dir, "scaler.joblib"))
            self.is_trained = True
            logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è: {e}")

def load_data_from_files(symbols: List[str], data_dir: str = "data/3year_minute_data") -> Dict[str, pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤"""
    logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–æ–≤...")
    
    all_data = {}
    
    for symbol in symbols:
        file_path = os.path.join(data_dir, f"{symbol}_3year_minute.csv")
        
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                df['begin'] = pd.to_datetime(df['begin'])
                df['end'] = pd.to_datetime(df['end'])
                
                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                df = df.drop_duplicates(subset=['begin']).reset_index(drop=True)
                
                all_data[symbol] = df
                logger.info(f"‚úÖ {symbol}: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {symbol}: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    return all_data

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    logger.info("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø ML –ú–û–î–ï–õ–ï–ô")
    logger.info("=" * 50)
    
    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    symbols = ['GAZP', 'SBER', 'PIKK', 'IRAO', 'SGZH']
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    all_data = load_data_from_files(symbols)
    
    if not all_data:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    trainer = FastMLTrainer()
    unsupervised_trainer = UnsupervisedTrainer()
    indicators = TechnicalIndicators()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    trained_models = {}
    
    for symbol, df in all_data.items():
        logger.info(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {symbol}...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        df_with_features = indicators.create_features(df)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        X, y, feature_columns = trainer.prepare_data(df_with_features)
        
        if len(X) > 100:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
            models = trainer.train_models_fast(X, y, symbol, feature_columns)
            trained_models[symbol] = {
                'models': models,
                'data': df_with_features
            }
        else:
            logger.warning(f"‚ö†Ô∏è {symbol}: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(X)} –∑–∞–ø–∏—Å–µ–π)")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è
    if trained_models:
        logger.info("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É—á–∏—Ç–µ–ª—è...")
        all_training_data = []
        for model_data in trained_models.values():
            all_training_data.append(model_data['data'])
        
        combined_df = pd.concat(all_training_data, ignore_index=True)
        unsupervised_trainer.train_unsupervised_models(combined_df)
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    summary_data = []
    
    for symbol, model_data in trained_models.items():
        models = model_data['models']
        
        for model_name, model_info in models.items():
            summary_data.append({
                'Symbol': symbol,
                'Model': model_name,
                'R2': f"{model_info['r2']:.4f}",
                'MAE': f"{model_info['mae']:.4f}",
                'CV_Mean': f"{model_info['cv_mean']:.4f}",
                'CV_Std': f"{model_info['cv_std']:.4f}",
                'Training_Time': f"{model_info['training_time']:.2f}s"
            })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É
    if summary_data:
        results_df = pd.DataFrame(summary_data)
        results_df.to_csv('model_training_results.csv', index=False)
        
        logger.info("\nüìã –°–í–û–î–ö–ê –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô:")
        logger.info("=" * 50)
        print(results_df.to_string(index=False))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        r2_scores = [float(r['R2']) for r in summary_data]
        mae_scores = [float(r['MAE']) for r in summary_data]
        
        logger.info(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"  –°—Ä–µ–¥–Ω–∏–π R¬≤: {np.mean(r2_scores):.4f}")
        logger.info(f"  –°—Ä–µ–¥–Ω–∏–π MAE: {np.mean(mae_scores):.4f}")
        logger.info(f"  –õ—É—á—à–∏–π R¬≤: {np.max(r2_scores):.4f}")
        logger.info(f"  –•—É–¥—à–∏–π R¬≤: {np.min(r2_scores):.4f}")
        
        # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        logger.info(f"\nüèÜ –õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–û –°–ò–ú–í–û–õ–ê–ú:")
        for symbol in symbols:
            symbol_results = [r for r in summary_data if r['Symbol'] == symbol]
            if symbol_results:
                best_model = max(symbol_results, key=lambda x: float(x['R2']))
                logger.info(f"  {symbol}: {best_model['Model']} (R¬≤={best_model['R2']})")
    
    logger.info(f"\nüíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:")
    logger.info(f"  üìÅ trained_models/ - –æ–±—É—á–µ–Ω–Ω—ã–µ ML –º–æ–¥–µ–ª–∏")
    logger.info(f"  üìÅ unsupervised_models/ - –º–æ–¥–µ–ª–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è")
    logger.info(f"  üìÑ model_training_results.csv - —Å–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    logger.info(f"  üìÑ model_training.log - –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
    
    logger.info("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main()
